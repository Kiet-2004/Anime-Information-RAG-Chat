{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-10T01:36:33.430113Z",
     "iopub.status.busy": "2025-03-10T01:36:33.429551Z",
     "iopub.status.idle": "2025-03-10T01:36:39.343704Z",
     "shell.execute_reply": "2025-03-10T01:36:39.342814Z",
     "shell.execute_reply.started": "2025-03-10T01:36:33.430074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu transformers torch huggingface_hub\n",
    "from IPython.display import clear_output\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:36:40.904921Z",
     "iopub.status.busy": "2025-03-10T01:36:40.904573Z",
     "iopub.status.idle": "2025-03-10T01:36:41.310154Z",
     "shell.execute_reply": "2025-03-10T01:36:41.309328Z",
     "shell.execute_reply.started": "2025-03-10T01:36:40.904891Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:36:48.575550Z",
     "iopub.status.busy": "2025-03-10T01:36:48.575265Z",
     "iopub.status.idle": "2025-03-10T01:36:55.895595Z",
     "shell.execute_reply": "2025-03-10T01:36:55.894966Z",
     "shell.execute_reply.started": "2025-03-10T01:36:48.575528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:37:01.230534Z",
     "iopub.status.busy": "2025-03-10T01:37:01.229965Z",
     "iopub.status.idle": "2025-03-10T01:37:01.414671Z",
     "shell.execute_reply": "2025-03-10T01:37:01.413866Z",
     "shell.execute_reply.started": "2025-03-10T01:37:01.230502Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/anime-dataset/result.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    anime_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:37:02.828565Z",
     "iopub.status.busy": "2025-03-10T01:37:02.828282Z",
     "iopub.status.idle": "2025-03-10T01:39:08.665339Z",
     "shell.execute_reply": "2025-03-10T01:39:08.664533Z",
     "shell.execute_reply.started": "2025-03-10T01:37:02.828544Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load embedding model (same as in the Colab notebook)\n",
    "embedding_model = AutoModel.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Load Mistral model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T02:07:33.944836Z",
     "iopub.status.busy": "2025-03-10T02:07:33.944520Z",
     "iopub.status.idle": "2025-03-10T02:07:33.953027Z",
     "shell.execute_reply": "2025-03-10T02:07:33.952165Z",
     "shell.execute_reply.started": "2025-03-10T02:07:33.944812Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = embedding_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze().cpu().numpy()\n",
    "    return sentence_embedding\n",
    "\n",
    "# Build Faiss Index\n",
    "def build_faiss_index(anime_data):\n",
    "    dimension = 384  \n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    embeddings = np.array([get_embedding(text) for text in tqdm(anime_data)], dtype=np.float32)\n",
    "    index.add(embeddings)\n",
    "    return index, embeddings\n",
    "\n",
    "def search_faiss(query, anime_data, index, top_k=3):\n",
    "    query_embedding = np.array([get_embedding(query)], dtype=np.float32)\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    return [anime_data[i] for i in indices[0]]\n",
    "\n",
    "def generate_response(context, query):\n",
    "    prompt = f\"You are an anime expert. Answer based on the following data:\\n{context}\\n\\nUser query: {query}\\nAnswer: \"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**inputs, max_new_tokens=300, do_sample=True)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response[response.index(\"Answer:\"):]\n",
    "\n",
    "def chatbot(anime_data, index):\n",
    "    print(\"Welcome to the Anime Chatbot! Type 'exit' to stop.\")\n",
    "    while True:\n",
    "        user_input = input(\"Ask anything about anime: \").strip()\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        retrieved_data = search_faiss(user_input, anime_data, index)\n",
    "        context = \"\\n\".join(retrieved_data)\n",
    "        response = generate_response(context, user_input)\n",
    "        print(f\"\\n{response}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:39:15.885083Z",
     "iopub.status.busy": "2025-03-10T01:39:15.884720Z",
     "iopub.status.idle": "2025-03-10T01:53:44.478742Z",
     "shell.execute_reply": "2025-03-10T01:53:44.478064Z",
     "shell.execute_reply.started": "2025-03-10T01:39:15.885053Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    index = faiss.read_index(\"anime_faiss.index\")\n",
    "except:\n",
    "    index, _ = build_faiss_index(anime_data)\n",
    "    faiss.write_index(index, \"anime_faiss.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-10T01:54:28.888087Z",
     "iopub.status.busy": "2025-03-10T01:54:28.887721Z",
     "iopub.status.idle": "2025-03-10T01:57:54.175837Z",
     "shell.execute_reply": "2025-03-10T01:57:54.175072Z",
     "shell.execute_reply.started": "2025-03-10T01:54:28.888061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "chatbot(anime_data, index)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6826121,
     "sourceId": 10974903,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
